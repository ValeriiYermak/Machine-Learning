{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error, root_mean_squared_error  \n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Download data for analise\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% some information about data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% some more information about data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean from the anomalies\n",
    "columns_to_check = ['AveRooms', 'AveBedrms', 'AveOccup', 'Population']\n",
    "\n",
    "# calculate z-scores\n",
    "z_scores = df_cleaned[columns_to_check].apply(zscore)\n",
    "\n",
    "#Identify anomalies: where z_score >3 or <-3\n",
    "outliers_mask = (np.abs(z_scores)> 3).any(axis=1)\n",
    "\n",
    "# remote lines from the anomalies\n",
    "\n",
    "df_cleaned = df_cleaned[~outliers_mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_martix = df_cleaned.corr()\n",
    "print(corr_martix['AveRooms'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=df_cleaned.drop(columns=['AveBedrms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_cleaned.drop(columns=['MedHouseVal'])\n",
    "y = df_cleaned['MedHouseVal']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model of linear regression\n",
    "\n",
    "model_lin = LinearRegression()\n",
    "model_lin.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred_lin = model_lin.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of the model\n",
    "\n",
    "r_sq_upd_lin = model_lin.score(X_train_scaled, y_train)\n",
    "mae_upd_lin = mean_absolute_error(y_test, y_pred_lin)\n",
    "mape_upd_lin = mean_absolute_percentage_error(y_test, y_pred_lin)\n",
    "print()\n",
    "print('Linear Regression')\n",
    "print(f\"R²: {r_sq_upd_lin:.4f} | MAE: {mae_upd_lin:.4f} | MAPE: {mape_upd_lin:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Polynomial Regression\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "y_pred_poly = model_poly.predict(X_test_poly)\n",
    "\n",
    "r2_poly = model_poly.score(X_train_poly, y_train)\n",
    "mae_poly = mean_absolute_error(y_test, y_pred_poly)\n",
    "mape_poly = mean_absolute_percentage_error(y_test, y_pred_poly)\n",
    "print()\n",
    "print('Polynomial Regression (level 2):')\n",
    "print(f'R2: {r2_poly:.2f} | MAE: {mae_poly:.2f} | MAPE: {mape_poly:.2f}')\n",
    "\n",
    "\n",
    "# %% \n",
    "# Plot of actual vs predicted values ​​for each model\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Linear Regression\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, y_pred_lin, alpha=0.5, label='Prediction vs Real')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "plt.title(\"Linear Regression: Real vs Predicted\")\n",
    "plt.xlabel(\"Real Features\")\n",
    "plt.ylabel(\"Predicted Features\")\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Regression\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test, y_pred_poly, alpha=0.5, label='Prediction vs Real')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "plt.title(\"Polynomial Regression: Real vs Predicted\")\n",
    "plt.xlabel(\"Real Features\")\n",
    "plt.ylabel(\"Predicted Features\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Linear Regression\n",
    "plt.subplot(1, 2, 1)\n",
    "residuals_lin = y_test - y_pred_lin\n",
    "sns.scatterplot(x=y_pred_lin, y=residuals_lin, alpha=0.5, label='Rating – Real')\n",
    "plt.hlines(0, xmin=y_pred_lin.min(), xmax=y_pred_lin.max(), colors='r', linestyles='--', label='Zero error')\n",
    "plt.title(\"Linear Regression: Residual errors\")\n",
    "plt.xlabel(\"Predicted Features\")\n",
    "plt.ylabel(\"Residual errors\")\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Regression\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals_poly = y_test - y_pred_poly\n",
    "sns.scatterplot(x=y_pred_poly, y=residuals_poly, alpha=0.5, label='Rating – Real')\n",
    "plt.hlines(0, xmin=y_pred_poly.min(), xmax=y_pred_poly.max(), colors='r', linestyles='--', label='Zero error')\n",
    "plt.title(\"Polynomial Regression: Residual errors\")\n",
    "plt.xlabel(\"Predicted Features\")\n",
    "plt.ylabel(\"Residual errors\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison plot of R2, MAE, MAPE for both models\n",
    "metrics = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Polynomial Regression'],\n",
    "    'R2': [r_sq_upd_lin, r2_poly],\n",
    "    'MAE': [mae_upd_lin, mae_poly],\n",
    "    'MAPE': [mape_upd_lin, mape_poly]\n",
    "})\n",
    "\n",
    "# Prepare of plot\n",
    "metrics.set_index('Model', inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# R2\n",
    "metrics['R2'].plot(kind='bar', ax=ax[0], color=['blue', 'green'])\n",
    "ax[0].set_title('R2: Comparison of models')\n",
    "ax[0].set_ylabel('R2')\n",
    "\n",
    "# MAE\n",
    "metrics['MAE'].plot(kind='bar', ax=ax[1], color=['blue', 'green'])\n",
    "ax[1].set_title('MAE: Comparison of models')\n",
    "ax[1].set_ylabel('MAE')\n",
    "\n",
    "# MAPE\n",
    "metrics['MAPE'].plot(kind='bar', ax=ax[2], color=['blue', 'green'])\n",
    "ax[2].set_title('MAPE: Comparison of models')\n",
    "ax[2].set_ylabel('MAPE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Comparison\n",
    "By comparing the metrics (R², MAE, MAPE) of the two models:\n",
    "If the Polynomial Regression model shows a higher R² (e.g. from 0.65 to 0.70) and lower MAE/MAPE, it suggests that the model is better capturing the nonlinear relationships between the features and the target variable.\n",
    "However, if the performance gains are marginal or the test error increases while train error drops, this may indicate overfitting.\n",
    "Polynomial regression can improve performance when there is evidence of non-linearity in the data, but it also increases the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual Plot Interpretation\n",
    "In a good model, residuals (errors) should be randomly scattered around zero without clear patterns.\n",
    "For Linear Regression:\n",
    "If residuals form a curved pattern, it means the model is underfitting — it's not capturing nonlinear trends.\n",
    "For Polynomial Regression:\n",
    "If residuals appear more centered and random, it suggests the model fits the data better.\n",
    "Polynomial regression likely reduces systematic errors, indicating better model flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of Real vs Predicted\n",
    "The closer the predicted values lie to the diagonal line (perfect prediction), the better the model.\n",
    "If Polynomial Regression points cluster more tightly around the line than in Linear Regression, it's a sign of better accuracy.\n",
    "The scatter plots confirm whether the model captures the variance in real outcomes well or consistently over/underestimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Impact of Preprocessing\n",
    "Outlier removal and feature scaling clearly improved model metrics.\n",
    "Removing highly correlated features helped avoid multicollinearity, which can inflate the variance of coefficient estimates.\n",
    "Proper data preprocessing is crucial — often as important as the model choice itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting Awareness\n",
    "If polynomial regression does well on training data but worse on test data, it's a sign of overfitting.\n",
    "We can address this with regularization (e.g., Ridge or Lasso regression) or by reducing the degree of the polynomial."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
